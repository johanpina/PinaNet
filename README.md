# üß¨ PinaNet: Deep Learning Transposable Element Annotator

**PinaNet** es una herramienta bioinform√°tica de alto rendimiento basada en **Deep Learning** para la detecci√≥n y anotaci√≥n autom√°tica de Elementos Transponibles (TEs) en secuencias gen√≥micas crudas (FASTA).

El sistema utiliza una arquitectura h√≠brida de √∫ltima generaci√≥n que combina la capacidad de representaci√≥n de **DNABERT-2** (un modelo de lenguaje pre-entrenado en ADN) con redes neuronales recurrentes bidireccionales (**BiLSTM**) para capturar el contexto secuencial y estructural de los TEs.

---

## üöÄ Caracter√≠sticas Principales

* **Arquitectura H√≠brida Avanzada:** Integra DNABERT-2 (para embeddings ricos de k-mers) + BiLSTM (para memoria secuencial) + Clasificador Lineal.
* **Soporte Multi-GPU Autom√°tico:** Detecta y utiliza autom√°ticamente todas las GPUs disponibles (DataParallel) para dividir la carga de trabajo y acelerar la inferencia exponencialmente.
* **Inferencia Vectorizada:** Utiliza operaciones de matrices (NumPy/PyTorch) y precisi√≥n mixta (FP16) para el post-procesamiento, eliminando los cuellos de botella de la CPU.
* **3 Niveles de Clasificaci√≥n:**
    * **Binario:** Detecci√≥n de presencia/ausencia (TE vs Background).
    * **Orden:** Clasificaci√≥n taxon√≥mica general (ej. LTR, LINE, SINE, DNA).
    * **Superfamilia:** Clasificaci√≥n taxon√≥mica detallada (ej. Gypsy, Copia, Mutator, etc.).
* **Estrategia "Mega-Chunks":** Procesa el genoma en fragmentos masivos configurables (ej. 1MB - 5MB) para saturar la memoria VRAM y minimizar la sobrecarga de comunicaci√≥n.
* **Salida Est√°ndar:** Genera archivos **GFF3** compatibles con IGV, JBrowse y otros visores gen√≥micos.

---

## üõ†Ô∏è Instalaci√≥n

### 1. Prerrequisitos
* **Python 3.9** o superior.
* (Recomendado) GPU NVIDIA con drivers CUDA instalados para inferencia r√°pida.
* Git.

### 2. Clonar el Repositorio
```bash
git clone https://github.com/TU_USUARIO/PinaNet.git
cd PinaNet
```

### 3. Crear Entorno Virtual
Se recomienda aislar las dependencias para evitar conflictos:

```bash
# Crear entorno
python -m venv venv

# Activar en Linux/Mac
source venv/bin/activate

# Activar en Windows
.\venv\Scripts\activate
```

### 4. Instalar Dependencias
```bash
pip install -r requirements.txt
```
*(Aseg√∫rese de que `torch`, `transformers`, `biopython`, `typer`, `pandas`, `numpy` y `tqdm` est√©n instalados).*

---

## üìÇ Configuraci√≥n de Modelos

Debido al gran tama√±o de los pesos neuronales, los modelos entrenados **no se incluyen** en el control de versiones de Git. Debes copiar tus carpetas directamente de los archivos del servidor o solicitarlos al owner del proyecto.

La estructura de carpetas debe verse **exactamente** as√≠ para que el software los reconozca:

```text
PinaNet/
‚îú‚îÄ‚îÄ Te_annotator.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ binary/            <-- Archivos del modelo Binario
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ order/             <-- Archivos del modelo de Orden
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ superfamilies/     <-- Archivos del modelo de Superfamilia
‚îÇ       ‚îú‚îÄ‚îÄ config.json
‚îÇ       ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ ...
```

**Nota Importante:** Aseg√∫rate de que cada carpeta contenga, como m√≠nimo, el archivo de configuraci√≥n `config.json` y los pesos del modelo `pytorch_model.bin`.

---

## üíª Uso

El programa se ejecuta desde la l√≠nea de comandos (CLI). La sintaxis b√°sica es:

```bash
python Te_annotator.py [ARGUMENTOS] [OPCIONES]
```

### Argumentos Principales

| Argumento | Descripci√≥n | Requerido |
| :--- | :--- | :---: |
| `fasta_file` | Ruta al archivo de entrada (`.fasta`, `.fa`, `.fna`). | ‚úÖ |
| `output_gff` | Ruta donde se guardar√° el archivo de anotaci√≥n (`.gff3`). | ‚úÖ |

### Opciones y Par√°metros

| Opci√≥n | Comando | Descripci√≥n | Default |
| :--- | :--- | :--- | :--- |
| **Nivel** | `--level` | Nivel de clasificaci√≥n: `binary`, `order`, `superfamilies`. | `binary` |
| **Chunk Size** | `--chunk-size` | Tama√±o del fragmento de genoma a procesar en memoria (pares de bases). **Aumentar para mayor velocidad, disminuir si hay error de memoria.** | `200000` |
| **Workers** | `--num-workers` | Hilos de CPU para cargar datos. Se recomienda mantener bajo (2) ya que la inferencia GPU es muy r√°pida. | `4` |
| **Device** | `--device` | Dispositivo de ejecuci√≥n: `cuda` (GPU) o `cpu`. | `cuda` |

---

## üß™ Ejemplos de Ejecuci√≥n

### 1. Detecci√≥n Binaria (R√°pida)
Escanea el genoma usando un chunk grande (2MB) para m√°xima velocidad en GPUs con buena VRAM (ej. 24GB+).

```bash
python Te_annotator.py \
    ./test/genoma_maiz.fasta \
    ./resultados/deteccion_binaria.gff3 \
    --level binary \
    --chunk-size 2000000 \
    --num-workers 2
```

### 2. Clasificaci√≥n por √ìrdenes (Equilibrada)
Configuraci√≥n est√°ndar para GPUs de rango medio (12GB - 16GB VRAM). Chunk de 1MB.

```bash
python Te_annotator.py \
    ./test/genoma_arroz.fasta \
    ./resultados/clasificacion_ordenes.gff3 \
    --level order \
    --chunk-size 1000000 \
    --device cuda
```

### 3. Clasificaci√≥n Fina (Segura)
El an√°lisis m√°s detallado. Si tienes poca VRAM libre, usa el chunk por defecto (200kb).

```bash
python Te_annotator.py \
    ./test/genoma_desconocido.fasta \
    ./resultados/full_annotation.gff3 \
    --level superfamilies \
    --chunk-size 200000
```

---

## üìä Formato de Salida (GFF3)

El archivo generado sigue el est√°ndar **GFF3** (Generic Feature Format versi√≥n 3). Ejemplo de salida:

```gff
##gff-version 3
chr1    DNABERT2    LTR 10500   12400   .   +   .   ID=LTR_10500_12400;Name=LTR_prediction
chr1    DNABERT2    LINE    15000   15800   .   +   .   ID=LINE_15000_15800;Name=LINE_prediction
```

* **Columna 1 (SeqID):** ID de la secuencia (cromosoma/contig).
* **Columna 2 (Source):** Fuente (`DNABERT2`).
* **Columna 3 (Type):** Tipo de TE (Predicci√≥n del modelo, ej. `LTR`).
* **Columna 4-5 (Start-End):** Coordenadas 1-based.
* **Columna 9 (Attributes):** ID √∫nico y metadatos para visualizaci√≥n.

---

## ‚öôÔ∏è Arquitectura del Sistema

PinaNet resuelve el problema de la longitud de entrada limitada de los modelos tipo BERT mediante una estrategia de **"Divide y Vencer√°s"** optimizada:

1.  **Mega-Chunking:** El genoma se divide en fragmentos grandes (ej. 1MB - 2MB) que se cargan en la VRAM de golpe.
2.  **Sliding Window Paralelo:** Cada Mega-Chunk contiene miles de ventanas de 512bp. Estas se distribuyen autom√°ticamente entre todas las GPUs disponibles.
3.  **Inferencia H√≠brida (FP16):**
    * **DNABERT-2:** Extrae caracter√≠sticas profundas de la secuencia de ADN.
    * **BiLSTM:** Analiza el contexto secuencial.
4.  **Reconstrucci√≥n Vectorizada:** Las predicciones se decodifican usando m√°scaras booleanas de NumPy, evitando bucles lentos de Python y permitiendo procesar millones de bases por segundo.

---

## ‚ö†Ô∏è Soluci√≥n de Problemas Comunes

* **Error `CUDA Out of memory`:** Est√°s intentando procesar un fragmento demasiado grande para tu GPU. **Soluci√≥n:** Reduce el par√°metro `--chunk-size`. Prueba bajando de `1000000` a `200000`.
* **Error `Model not found`:** Verifica que hayas copiado las carpetas `binary`, `order` y `superfamilies` dentro de la carpeta `models/` y que los nombres coincidan exactamente.
* **Advertencias de `Triton / Flash Attention`:** Son normales si no tienes la arquitectura de GPU m√°s reciente (Hopper/Ampere). El sistema est√° configurado para cambiar autom√°ticamente a una implementaci√≥n compatible.

---

## üìù Licencia

Este proyecto est√° bajo la licencia [MIT](LICENSE).

---
**Desarrollado por Johan S. Pi√±a - 2025**